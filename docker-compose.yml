version: '3.8'

services:
  vault:
    image: hashicorp/vault:latest
    container_name: vault
    cap_add:
      - IPC_LOCK
    ports:
      - "8200:8200"
    networks:
      - spark-airflow-network
    volumes:
      - ./vault/config:/vault/config
      - ./vault/data:/vault/file
      - ./vault/tls:/vault/tls
      - ./vault/scripts:/vault/scripts
      - ./shared-secrets:/vault/shared-secrets
    command: /bin/sh -c "vault server -config=/vault/config/vault-config.hcl"
    restart: unless-stopped

  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    hostname: spark-master
    ports:
      - "8080:8080"
    networks:
      - spark-airflow-network
    environment:
      - SPARK_MODE=master
    volumes:
      - ./data_processing/jobs:/opt/bitnami/spark/jobs

  spark-worker-1:
    image: bitnami/spark:latest
    container_name: spark-worker-1
    hostname: spark-worker-1
    depends_on:
      - spark-master
    networks:
      - spark-airflow-network
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    volumes:
      - ./data_processing/jobs:/opt/bitnami/spark/jobs

  spark-worker-2:
    image: bitnami/spark:latest
    container_name: spark-worker-2
    hostname: spark-worker-2
    depends_on:
      - spark-master
    networks:
      - spark-airflow-network
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    volumes:
      - ./data_processing/jobs:/opt/bitnami/spark/jobs

  spark-worker-3:
    image: bitnami/spark:latest
    container_name: spark-worker-3
    hostname: spark-worker-3
    depends_on:
      - spark-master
    networks:
      - spark-airflow-network
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    volumes:
      - ./data_processing/jobs:/opt/bitnami/spark/jobs

  # Airflow services
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    networks:
      - spark-airflow-network

  redis:
    image: redis:latest
    networks:
      - spark-airflow-network

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: airflow-webserver
    depends_on:
      - postgres
      - redis
      - airflow-init
    env_file:
      - ./data_extraction/airflow.env 
    ports:
      - "8081:8080"
    networks:
      - spark-airflow-network
    command: webserver
    volumes:
      - ./data_extraction/dags:/opt/airflow/dags
      - ./data_processing/jobs:/opt/bitnami/spark/jobs
      - ./shared-secrets:/shared-secrets

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: airflow-scheduler
    depends_on:
      - postgres
      - redis
      - airflow-webserver
    env_file:
      - ./data_extraction/airflow.env 
    networks:
      - spark-airflow-network
    command: scheduler
    volumes:
      - ./data_extraction/dags:/opt/airflow/dags
      - ./data_processing/jobs:/opt/bitnami/spark/jobs
      - ./shared-secrets:/shared-secrets

  airflow-worker:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: airflow-worker
    depends_on:
      - postgres
      - redis
      - airflow-webserver
    env_file:
      - ./data_extraction/airflow.env  
    networks:
      - spark-airflow-network
    command: celery worker
    volumes:
      - ./data_extraction/dags:/opt/airflow/dags
      - ./data_processing/jobs:/opt/bitnami/spark/jobs
      - ./shared-secrets:/shared-secrets  # Updated volume

  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: airflow-init
    depends_on:
      - postgres
      - redis
    env_file:
      - ./data_extraction/airflow.env   
    entrypoint: /bin/bash -c "airflow db init && airflow users create --username admin --password admin --firstname Airflow --lastname Admin --role Admin --email admin@example.com"
    networks:
      - spark-airflow-network
    volumes:
      - ./data_extraction/dags:/opt/airflow/dags
      - ./data_processing/jobs:/opt/bitnami/spark/jobs
      - ./shared-secrets:/shared-secrets  # Updated volume

secrets:
  vault_token:
    external: true

networks:
  spark-airflow-network:
    driver: bridge

volumes:
  shared-secrets:
